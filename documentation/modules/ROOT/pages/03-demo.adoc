include::_attributes.adoc[]

= Solution Patterns: {title}
:sectnums:
:sectlinks:
:doctype: book

= See the Solution in Action

== Before running the solution

In order to reproduce the following solution, you will need the following resources:

* Access to a running {ocp} cluster with {aap} operator available through the Operator Hub  
* A subscription or trial license for {aap} (running on {ocp} cluster)  
* {rhsi} (version 2.x) RPM to be installed at the {rhel} machine (for Third-party Network A)  
* A few virtual machines to be used as third-party servers, with SSH server running  
* A machine or a virtual machine with {rhel} to simulate third-party network A, with access to the respective servers  
* A Linux machine or virtual machine with Podman version 4+ to simulate third-party network B, with access to the respective servers
* The following commands are expected: `kubectl`, `jq`, `awk`, `wget` and `podman`

== Walkthrough

=== Downloading resources needed

This walkthrough guide uses some files packaged in a tar ball, which must be downloaded and extracted locally. +
To download and extract the files needed, open a terminal and execute the following commands:

[.console-input]
[source,shell script]
----
wget https://raw.githubusercontent.com/fgiorgetti/solution-pattern-rhsi-aap/refs/heads/main/documentation/modules/ROOT/assets/resources.tar.gz
tar zxvf resources.tar.gz
----

=== Installing {aap}

We will just briefly explain how to install {aap} on an {ocp} cluster.
If you need further information, please refer to the https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html-single/installing_on_openshift_container_platform/index#proc-install-cli-aap-operatorinstalling-aap-operator-cli[official installation guide].

Before you start, notice that you will need a valid {aap} (AAP) subscription to proceed,
otherwise you won't be able to complete the AAP installation.

. Open the {ocp} console
. Inside the console, go to: Operators → OperatorHub
. Search for "Ansible Automation Platform" as shown below:
+
image::walkthrough_operator_hub.png[Operator Hub]
+
. Select the `Ansible Automation Platform` operator and click `*Install*`
+
image::walkthrough_operator_install.png[Operator Install]
+
. Using the default options, with `Installed namespace: *aap*`, click `*Install*` again
. Wait for the operator to be installed and ready
. Click on the installed operator, then find the `Ansible Application Platform` resource and click `*Create instance*`
+
image::walkthrough_aap_create.png[Operator Install]
+
. Enter the following Name, Service Type and Ingress type, then click `*Create*`:
+
image::walkthrough_aap_create_form.png[Operator Install]
+
. Wait for the `Ansible Automation Platform` (AAP) creation to complete (around 10 minutes)
. Once the pods are all running on the `*aap*` namespace, locate secret `*my-aap-admin-password*`
. Copy the `*password*` value, which will be used next to log into the AAP console
. Under "Networking → Routes", open the URL for route `*my-aap*`
. Login as admin using the password you copied earlier
. You will be prompted to *enter your subscription information*
. Once you supply your subscription details, you should be logged into the AAP console

=== Configuring your Ansible Automation Platform instance

In order to validate this solution pattern, we will need to configure the Ansible Automation Platform (AAP) instance first.

Since the goal is to demonstrate how {rhsi} (RHSI) helps you connect your AAP automation with managed hosts that are not reachable,
the AAP configuration used in this solution pattern is minimal.

Here is what we will configure:

* A project to run against all hosts
* Two inventories (one for each third-party network)
* Define the credentials
* Two job templates (one for each third-party network)
* A workflow job template (to run all third-party related job templates)

==== Create a project

The Ansible project that will be used in this solution pattern is a simple fork from the ansible-tower-samples repository
which includes an extra task that simply creates a directory under `*/tmp*` named `*created-by-aap*`.
This helps validate that AAP has actually connected and performed this respective task against the target host.

In the AAP console, perform the following tasks:

* Go to `*Projects*` on the left menu (under _Automation Execution_)
* Click `*Create project*`
* Enter the following data:
 ** Name: *third-party-networks*
 ** Organization: *Default*
 ** Execution environment: *Default execution environment*
 ** Source control type: *Git*
 ** Source control URL: https://github.com/fgiorgetti/ansible-tower-samples.git[*https://github.com/fgiorgetti/ansible-tower-samples.git*]
 ** Source control branch/tag/commit: *master*
* Click: *Create project*

==== Create the inventories

Now that the project has been created, we will create two inventories. One for each third-party network.

* Go to Infrastructure → Inventories on the left menu (under _Automation Execution_)
* Click: `*Create inventory → Create inventory*`
* Enter the following data:
 ** Name: *third-party-network-a*
 ** Organization: *Default*
 ** Click: *Create inventory*
* Repeat the same steps to create the inventory named: *third-party-network-b*

Next we need to add the target hosts for each inventory.

* Open the `*third-party-network-a*` inventory and click the `*Hosts*` tab
* Click the `*Create host*` button
* Enter the following data:
 ** Name: *server-a1*
 ** Set variables to (using YAML format):
+
*ansible_host: server-a1.net-a*
* Repeat the previous steps to create: *server-a2* and *server-a3*

Then we need to add the hosts for the second third-party network.

* Open the *third-party-network-b* inventory and click the *Hosts* tab
* Click the *Create host* button
* Enter the following data:
 ** Name: *server-b1*
 ** Set variables to (using YAML format):
+
*ansible_host: server-b1.net-b*
* Repeat the previous steps to create: *server-b2* and *server-b3*

As you may have noticed, our inventories rely that the servers will be reachable through the following hostnames:

* server-a1.net-a
* server-a2.net-a
* server-a3.net-a
* server-b1.net-b
* server-b2.net-b
* server-b3.net-b

At this point, these hosts cannot be resolved by AAP, but once we finish the setup of our solution, AAP should be
able to resolve and connect with these hosts.

==== Create the credentials

The inventories that will be used by Ansible have been defined, but we did not specify the credentials that Ansible
will use to connect with those servers through SSH. +
To keep it simple we will create two `*Machine*` (type) credentials. One for each third-party network.

* Go to `*Infrastructure → Credentials*` (under Automation Execution)
* Click `*Create credential*`
* Enter the following data:
 ** Name: *third-party-network-a*
 ** Organization: *Default*
 ** Credential type: *Machine*
 ** Username: *cloud-user*
 ** Password: *cloud-password-a*
 ** Click the `*Create credential*` button

Repeat the same steps to create a credential named *third-party-network-b*.

[NOTE]
====
Make sure you are using the correct credentials for your servers.
====

==== Create the job templates

We need to create two job templates.

They will basically tie the Ansible project to be executed with the inventory that defines the target hosts and
associate the credential to be used for each third-party network.

* Go to `*Templates*` (under Automation Execution) on the left menu
* Click the `*Create template*` button, then click `*Create job template*`
* Enter the following data:
 ** Name: *third-party-network-a*
 ** Inventory: *third-party-network-a*
 ** Project: *third-party-networks*
 ** Playbook (should be automatically set): *hello_world.yml*
 ** Credentials: *third-party-network-a*
 ** Click the `*Create job template*` button

Repeat the steps above for *third-party-network-b*.

After both _Job Templates_ have been created, we need to create a `*Workflow job template*`. +
It will be used to trigger both job templates at once.

* Go to `*Templates*` (under Automation Execution) on the left menu
* Click the `*Create template*` button, then click `*Create workflow job template*`
* Enter the following data:
 ** Name: third-party-networks
 ** Organization: Default
 ** Click the `*Create workflow job template*` button
 ** Click the `*Add step*` button
 ** Node type: Job Template
 ** Job Template: third-party-network-a
 ** Click the `*Next*` button
 ** Click the `*Finish*` button
 ** Click the  `*Add step*` button
 ** Node type: Job Template
 ** Job Template: third-party-network-b
 ** Click the `*Next*` button
 ** Click the `*Finish*` button
 ** Click the `*Save*` button

You should end up with something like shown in the workflow below:

image::walkthrough_aap_workflow_created.png[Workflow job template]

At this point, all the AAP configuration needed is done.

Now we need to install and configure *{rhsi}* to link the third-party networks with the {ocp} cluster.

=== Installing {rhsi} on {ocp}

[IMPORTANT]
====
THE FOLLOWING INSTRUCTIONS USE THE UPSTREAM VERSION OF RHSI (SKUPPER V2) +
ONCE RHSI V2 IS RELEASED, THIS WHOLE SECTION MUST BE UPDATED TO USE RHSI V2 INSTEAD.
====

We need to install *{rhsi}* in two separate namespaces. +
The reason for this is to ensure that each third-party network is connected to an isolated _Virtual Application Network_ (VAN).

Along with that we will also create _Network Policies_ to ensure that each namespace is only accessible to AAP,
so that the third-party servers exposed into the {ocp} cluster can only be reached internally by AAP.

==== Installing the *{rhsi}* controller

The RHSI controller is deployed to its own namespace and it has the ability to watch for RHSI resources across all namespaces
in the {ocp} cluster. +
Let's install the RHSI controller:

* Open a terminal
* Set the *KUBECONFIG* environment variable
 ** You must be logged in as a cluster administrator
* Create the `*skupper*` namespace, using:
+
[.console-input]
[source,shell script]
----
kubectl create namespace skupper
----
+
* The RHSI V2 controller can be installed using:
+
[.console-input]
[source,shell script]
----
kubectl -n skupper apply -f https://skupper.io/v2/install.yaml
----
+
* Now wait for the *skupper-controller* pod to be running on the *skupper* namespace:
+
[.console-input]
[source,shell script]
----
kubectl -n skupper wait --for=condition=Ready pod -l application=skupper-controller
----

==== Create namespaces for each third-party network

Once the RHSI controller is running, we need to apply the RHSI Custom Resources (CRs) for the RHSI controller to create the:

* Sites
* Listeners
* AccessTokens

The first thing we need to do is create the namespaces for third-party networks A and B. +
The namespaces will be named: `*net-a*` and `*net-b*` respectively.

These namespaces have already been used to compose the expected fully qualified hostnames
used in the AAP inventories, example given: `*server-a1.net-a*`.

Let's have a look at the custom resources we need to define these two RHSI sites. +
The resources can be found at `*./cloud/net-a*` and `*./cloud/net-b*` folders.

* `*Site*`
 ** A Site represents a separate RHSI instance
 ** You can only have a single RHSI site per namespace
 ** It is the main resource to be created
* `*AccessGrant*`
 ** Grants the permission for other sites to redeem AccessTokens to this site
 ** AccessTokens can be generated once the AccessGrant is ready
* `*Listener*`
 ** Represents an ingress to target workloads exposed in your Virtual Application Network (VAN)
 ** On OpenShift / Kubernetes they are realized as a Service
 ** Listeners must have a corresponding Connector available in some other Site linked to the VAN

Both sites set the *_linkAccess_* value to *_default_*. This will ensure that the default ingress method for the target
cluster will be used. +
On {ocp} clusters, a *Route* should be created, otherwise a *LoadBalancer* service will be created.
This ingress method is used to accept incoming links coming from other Sites.

The `*AccessGrant*` allows a single `*AccessToken*` to be redeemed and it must be redeemed within 30 minutes
from `*AccessGrant*` creation, otherwise it won't be valid.

Each site has a `*Listener*` for each target server expected by AAP.
The `*spec.host*` field determines the Service name that will be created on the respective namespace,
therefore the fully qualified service name will be composed by the spec.host field plus the namespace
name, matching the hostnames added to the inventories in AAP.

The `*spec.routingKey*` is used to determine the matching Connector.
So the RHSI sites created inside the third-party networks, must define the respective `*spec.routingKey*`.

Along with the RHSI resources, there are two _Network policies_ defined to add an extra security layer (one for each namespace),
preventing undesired internal access to your third-party network namespaces.
These `*NetworkPolicies*` allow ingress to the `*skupper-router*` pod only coming from pods running on the `*aap*` or the
self namespace (`*net-a*` or `*net-b*`).

To create the namespaces, the RHSI sites and the network policies, run:
[.console-input]
[source,shell script]
----
kubectl apply -f ./cloud/net-a/
kubectl apply -f ./cloud/net-b/
----


You can verify that your sites have been created running:

[.console-input]
[source,shell script]
----
kubectl -n net-a get pod,site
kubectl -n net-b get pod,site
----

And you should see an output similar to this one:

[.console-input]
----
NAME                                  READY   STATUS    RESTARTS   AGE
pod/skupper-router-78b5b8ddb5-h76sm   2/2     Running   0          53m

NAME                    STATUS   SITES IN NETWORK
site.skupper.io/net-a   OK


NAME                                  READY   STATUS    RESTARTS   AGE
pod/skupper-router-5f54dd4f88-24rzh   2/2     Running   0          54m

NAME                    STATUS   SITES IN NETWORK
site.skupper.io/net-b   OK
----

==== Preparing site bundles for third-party networks

A site bundle is a compressed file that contains a whole RHSI site definition to run outside of Kubernetes or OpenShift. +
They can be installed to run as a container, using Podman or Docker and also as an regular process on a {rhel} server,
which will require a local installation of the `*skupper-router*` _RPM_ package.

The site bundle is an easy approach to install a prepared site definition on a remote location, but you could also create
a non-Kubernetes site using the Skupper V2 CLI or a bootstrap container.

Here are the Custom Resources (CRs) needed to define the site bundles.
* `*Site*`
* `*Connector*`
* `*AccessToken*` (will be generated from `*AccessGrants*` created earlier)

In order to prepare a site bundle to be installed at the Third Party Networks, we will use the CRs located under
`*./internal/net-a*` and `*./internal/net-b*`.

Along with these CRs, we also need to generate `*AccessTokens*`, so that the bundles have the ability to establish
an RHSI link against the RHSI site running on the {ocp} cluster on namespaces `*net-a*` and `*net-b*` respectively.

To generate the `*AccessTokens*` we will need to extract some information from the `*AccessGrant*` created on both
`*net-a*` and `*net-b*` namespaces. +
To do it, execute the `*./scripts/generate-access-token.sh*` script, using:

[.console-input]
[source,shell script]
----
./scripts/generate-access-token.sh net-a > ./internal/net-a/20-token.yaml
./scripts/generate-access-token.sh net-b > ./internal/net-b/20-token.yaml
----

Now that all the CRs are in place, we must generate the site bundles, using:

[.console-input]
[source,shell script]
----
curl -s https://raw.githubusercontent.com/skupperproject/skupper/refs/heads/v2/cmd/bootstrap/bootstrap.sh | sh -s -- -p ./internal/net-a -b bundle
curl -s https://raw.githubusercontent.com/skupperproject/skupper/refs/heads/v2/cmd/bootstrap/bootstrap.sh | sh -s -- -p ./internal/net-b -b bundle
----

The bundles will be generated and their location can be found through a message that says:
"_Installation bundle available at_". Example given:

[.console-input]
----
Installation bundle available at: /home/my-user/.local/share/skupper/bundles/skupper-install-net-a.sh
----

=== Connecting the Third-party Networks

The last piece is to install the generated site-bundles on the respective servers for each target third-party network.

To install, you should just send the site bundle file: `*skupper-install-net-a.sh*` or `*skupper-install-net-b.sh*` to
the target server where the RHSI site will be installed, then execute it, for example:

[.console-input]
[source,shell script]
----
scp skupper-install-net-a.sh my-user@my-server-third-party-net-a:
./skupper-install-net-a.sh -n net-a
----

[.console-input]
[source,shell script]
----
scp skupper-install-net-b.sh my-user@my-server-third-party-net-b:
./skupper-install-net-b.sh -n net-b
----

[WARNING]
====
The commands above have been executed against an internal hostname that
represents the server where the RHSI site bundle will be installed and these
servers can reach the target hosts that will be managed by AAP. +
Update the commands to use your own hostnames or IP addresses.
====

[NOTE]
====
If you want your *Third Party Network A* site to run using a regular process and not a container, you must first
install the `*skupper-router*` _RPM_ package and then you should run the bundle installation script as:

[.console-input]
[source,shell script]
----
scp skupper-install-net-a.sh my-user@my-server-third-party-net-a:
./skupper-install-net-a.sh -n net-a -p systemd
----
====

Once both bundles have been installed, the three servers behind each third party network should be exposed and accessible
by AAP inside the {ocp} cluster.

=== Validating the scenario

Now the whole scenario has been deployed and the *_Workflow job template_* that was created on AAP should be able to run and
reach all servers.

Back to the AAP console, go to:

* `*Templates*` (under Automation Execution)
* Click the `*third-party-networks*` workflow job template
* Click the `*Launch template*` button
* Watch the workflow
* You should see a successful result

image::walkthrough_aap_workflow_success.png[Workflow job template successful]
