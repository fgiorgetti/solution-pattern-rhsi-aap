include::_attributes.adoc[]

= Solution Patterns: {title}
:sectnums:
:sectlinks:
:doctype: book

= See the Solution in Action

== Before running the solution

In order to reproduce the following solution, you will need the following resources:

* Access to a running {ocp} cluster with {aap} operator available through the Operator Hub  
* A subscription or trial license for {aap} (running on {ocp} cluster)  
* {rhsi} (version 2.x) RPM to be installed at the {rhel} machine (for Third-party Network A)  
* A few virtual machines to be used as third-party servers, with SSH server running  
* A machine or a virtual machine with {rhel} to simulate third-party network A, with access to the respective servers  
* A Linux machine or virtual machine with Podman version 4+ to simulate third-party network B, with access to the respective servers (optional)
* Install the Skupper CLI version 2.x
* The following commands are expected: `kubectl`, `jq`, `awk`, `wget` and `podman`

== Walkthrough

=== Installing the {rhsi} command line tool

You need to install the {rhsi} command line tool in order to generate AccessTokens to the connect your third-party
network sites with {rhsi} running in the cloud.

[.console-input]
[source,shell script]
----
curl https://skupper.io/install.sh | sh -s -- --version 2.0.0-preview-1
----

The script installs the command under your home directory. It prompts you to add the command to your path if necessary.

=== Installing {rhsi} on {ocp}

[IMPORTANT]
====
The following instructions use the upstream version of {rhsi} (Skupper V2), +
as at the time this solution pattern has been written, RHSI Version 2 has not yet been released. +
Once RHSI Version 2 is released, this whole section will be updated to use it instead.
====

We need to install *{rhsi}* in two separate namespaces. +
The reason for this is to ensure that each third-party network is connected to an isolated _Virtual Application Network_ (VAN).

Along with that we will also create _Network Policies_ to ensure that each namespace is only accessible by AAP, so that the third-party servers exposed into the {ocp} cluster can only be reached internally by AAP.

==== Installing the *{rhsi}* controller

The RHSI controller is deployed to its own namespace and it has the ability to watch for RHSI resources across all namespaces in the {ocp} cluster.

Let's install the RHSI controller:

* Open a terminal
* Set the *KUBECONFIG* environment variable
 ** You must be logged in as a cluster administrator
* Create the `*skupper*` namespace, using:
+
[.console-input]
[source,shell script]
----
kubectl create namespace skupper
----
+
* The RHSI V2 controller can be installed using:
+
[.console-input]
[source,shell script]
----
kubectl -n skupper apply -f https://skupper.io/v2/install.yaml
----
+
* Now wait for the *skupper-controller* pod to be running on the *skupper* namespace:
+
[.console-input]
[source,shell script]
----
kubectl -n skupper wait --for=condition=Ready pod -l application=skupper-controller
----

==== Create namespaces for each third-party network

Once the RHSI controller is running, we need to apply the RHSI Custom Resources (CRs) for the RHSI controller to create the:

* Sites
* Listeners
* AccessGrants

Just as a reminder, this walkthrough will only cover a single namespace on {ocp} and a single third-party network. In case you want to add two or more third-party networks, the instructions are basically the same.

The first thing we need to do is create the namespace for third-party network A. +
The namespace will be named: `*net-a*`.

This namespace will be used to compose the expected fully qualified hostnames to be used in the AAP inventory, example given: `*server-a1.net-a*`.

Let's have a look at the custom resources we need to define this RHSI site.

* `*Site*`
 ** A Site represents a separate RHSI instance
 ** You can only have a single RHSI site per namespace
 ** It is the main resource to be created
* `*AccessGrant*`
 ** Allows AccessTokens to be redeemed by other sites
 ** Can be configured to permit a certain amount of token redemptions
 ** Expire after pre-defined amount of time
 ** AccessTokens can be generated once the AccessGrant is ready
* `*Listener*`
 ** Represents an ingress to target workloads exposed in your Virtual Application Network (VAN)
 ** On OpenShift / Kubernetes they are realized as a Service
 ** Listeners must have a corresponding Connector available in some other Site linked to the VAN

The site definiton sets the *_linkAccess_* value to *_default_*. This will ensure that the default ingress method for the target cluster will be used. +
On {ocp} clusters, a *Route* should be created, otherwise a *LoadBalancer* service will be created.
This ingress method is used to accept incoming links, coming from other Sites.

The `*AccessGrant*` allows a single `*AccessToken*` to be redeemed and it must be redeemed within 30 minutes from `*AccessGrant*` creation, otherwise it won't be valid.

Each site has a `*Listener*` for each target server expected by AAP. +
The `*spec.host*` field determines the service name that will be created on the respective namespace,
therefore the fully qualified service name will be composed by the `spec.host` field plus the namespace name, matching the hostnames that will be added to the inventories in AAP.

The `*spec.routingKey*` is used to determine the matching Connector.
So the RHSI sites created inside the third-party networks, must define the respective `*spec.routingKey*`.

Along with the RHSI resources, a _Network policy_ will be defined, to add an extra security layer, preventing undesired internal access to your third-party network namespaces. +
This `*NetworkPolicy*` allows ingress to the `*skupper-router*` pod only coming from pods running on the `*aap*` or the self namespace `*net-a*`.

To create the namespace, the RHSI site and the network policy, run the following commands in your terminal that has access to the {ocp} cluster. The commands below will first create the resources in a local directory then apply them to the {ocp} cluster.

[.console-input]
[source,shell script]
----
# create a directory to hold all resources
mkdir -p cloud/net-a/

# create the resources needed
cat << EOF > cloud/net-a/00-ns.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: net-a
EOF

cat << EOF > cloud/net-a/10-site.yaml
apiVersion: skupper.io/v2alpha1
kind: Site
metadata:
  name: net-a
  namespace: net-a
spec:
  linkAccess: default
EOF

cat << EOF > cloud/net-a/20-accessgrant.yaml
apiVersion: skupper.io/v2alpha1
kind: AccessGrant
metadata:
  name: net-a-grant
  namespace: net-a
spec:
  redemptionsAllowed: 1
  expirationWindow: 30m
EOF

cat << EOF > cloud/net-a/30-listeners.yaml
---
apiVersion: skupper.io/v2alpha1
kind: Listener
metadata:
  name: server-a1
  namespace: net-a
spec:
  host: server-a1
  port: 22
  routingKey: server-a1
  type: tcp
---
apiVersion: skupper.io/v2alpha1
kind: Listener
metadata:
  name: server-a2
  namespace: net-a
spec:
  host: server-a2
  port: 22
  routingKey: server-a2
  type: tcp
---
apiVersion: skupper.io/v2alpha1
kind: Listener
metadata:
  name: server-a3
  namespace: net-a
spec:
  host: server-a3
  port: 22
  routingKey: server-a3
  type: tcp
EOF

cat << EOF > cloud/net-a/40-networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: network-policy-aap
  namespace: net-a
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: skupper-router
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchExpressions:
        - key: 'kubernetes.io/metadata.name'
          operator: In
          values: ["aap", "net-a"]
  egress:
  - {}
EOF

kubectl apply -f ./cloud/net-a/
----


You can verify that your site has been created running:

[.console-input]
[source,shell script]
----
kubectl -n net-a get pod,site
----

And you should see an output similar to this one:

[.console-input]
----
NAME                                  READY   STATUS    RESTARTS   AGE
pod/skupper-router-78b5b8ddb5-h76sm   2/2     Running   0          53m

NAME                    STATUS   SITES IN NETWORK
site.skupper.io/net-a   OK
----

==== Preparing site bundles for the third-party network

A site bundle is a compressed file that contains the whole RHSI site definition to run outside of Kubernetes or OpenShift. +
They can be installed to run as a container, using Podman or Docker and also as a regular process on a {rhel} server, which will require a local installation of the `*skupper-router*` _RPM_ package.

The site bundle is an easy approach to install a prepared site definition on a remote location, but you could also create a non-Kubernetes site using the {rhsi} V2 CLI or a bootstrap container.

Here are the Custom Resources (CRs) needed to define the site bundles.

* `*Site*`
* `*Connectors*`
* `*AccessToken*` (will be generated from `*AccessGrants*` created earlier)

In order to prepare a site bundle to be installed at the Third Party Networks, we will create the Custom Resources (CRs) needed along with an `*AccessTokens*` that will be extracted from the `*net-a*` namespace running on the {ocp} cluster.

To do it, execute the following commands in a terminal that has access to `*net-a*` namespace running on your {ocp} cluster:

[.console-input]
[source,shell script]
----
mkdir -p internal/net-a

cat << EOF > internal/net-a/10-site.yaml
apiVersion: skupper.io/v2alpha1
kind: Site
metadata:
  name: net-a
EOF

cat << EOF > internal/net-a/20-connectors.yaml
---
apiVersion: skupper.io/v2alpha1
kind: Connector
metadata:
  name: server-a1
spec:
  host: 192.168.8.101
  port: 22
  routingKey: server-a1
---
apiVersion: skupper.io/v2alpha1
kind: Connector
metadata:
  name: server-a2
spec:
  host: 192.168.8.102
  port: 22
  routingKey: server-a2
---
apiVersion: skupper.io/v2alpha1
kind: Connector
metadata:
  name: server-a3
spec:
  host: 192.168.8.103
  port: 22
  routingKey: server-a3
EOF

kubectl -n net-a get accessgrant net-a-grant -o template --template '
apiVersion: skupper.io/v2alpha1
kind: AccessToken
metadata:
  name: net-a-token
spec:
  code: "{{ .status.code }}"
  ca: {{ printf "%q" .status.ca }}
  url: "{{ .status.url }}"
' > internal/net-a/30-accesstoken.yaml
----

Now that all the CRs are in place, we must generate the site bundle, using:

[.console-input]
[source,shell script]
----
curl -s https://raw.githubusercontent.com/skupperproject/skupper/refs/heads/v2/cmd/bootstrap/bootstrap.sh | sh -s -- -p ./internal/net-a -b bundle
----

The bundle will be generated and its location can be found through a message that says:
"_Installation bundle available at_". Example given:

[.console-input]
----
Installation bundle available at: /home/my-user/.local/share/skupper/bundles/skupper-install-net-a.sh
----

=== Connecting the Third-party Network

The last piece to complete the {rhsi} setup is to install the generated site-bundles on the respective server used to reach Third-party Network A (`*net-a*`) hosts.

To install, you should just send the site bundle file: `*skupper-install-net-a.sh*` to the target server where the RHSI site will be installed, then execute it, for example:

[.console-input]
[source,shell script]
----
scp skupper-install-net-a.sh my-user@my-server-third-party-net-a:
ssh my-user@my-server-third-party-net-a ./skupper-install-net-a.sh -n net-a
----

[WARNING]
====
The commands above have been executed against internal hosts that
represent the servers where the RHSI site bundle will be installed
and these servers can reach the target hosts that will be managed by AAP. +
Update the commands to use your own hostnames or IP addresses.
====

[NOTE]
====
If you want your *Third Party Network A* site to run using a regular process and not a container, you must first install the `*skupper-router*` _RPM_ package and then you should run the bundle installation script as:

[.console-input]
[source,shell script]
----
scp skupper-install-net-a.sh my-user@my-server-third-party-net-a:
ssh my-user@my-server-third-party-net-a dnf -y install skupper-router
ssh my-user@my-server-third-party-net-a ./skupper-install-net-a.sh -n net-a -p systemd
----
====

Once the bundle has been installed, the three servers behind the third party network should be exposed and accessible by AAP inside the {ocp} cluster.

Now let's have a quick look at what must be done to install and configure {aap}.

=== Installing {aap}

We will just briefly explain what is expected from your {aap} installation on the {ocp} cluster.

The {aap} installation is expected to run in the "*aap*" namespace, using the {aap} Operator and an instance of the "Ansible Automation Platform" resource must be created.

If you need further information, please refer to the https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html-single/installing_on_openshift_container_platform/index#proc-install-cli-aap-operatorinstalling-aap-operator-cli[official installation guide].

=== Configuring your Ansible Automation Platform instance

In order to validate this solution pattern, we will need to configure the Ansible Automation Platform (AAP) instance first.

[NOTE]
====
Since the goal is to demonstrate how {rhsi} (RHSI) helps you connect your AAP automation with managed hosts that are not reachable, the AAP configuration used in this solution pattern is minimal.
====

Here is what we will configure:

* A project to run against all hosts
* An inventory (to represent the Third-party Network A hostnames)
* Define the credential
* A job template (for Third-party Network A)

==== Create a project

The Ansible project that will be used in this solution pattern is a simple fork from the ansible-tower-samples repository which includes an extra task that simply creates a directory under `*/tmp*` named `*created-by-aap*`. +
This helps validate that {aap} has actually connected and performed this respective task against the target host.

In the AAP console, create a project using the following information:

 * Source control type: *Git*
 * Source control URL: https://github.com/fgiorgetti/ansible-tower-samples.git[*https://github.com/fgiorgetti/ansible-tower-samples.git*]
 * Source control branch/tag/commit: *master*

==== Create the inventory

Now that the project has been created, we will create an inventory, to represent the hostnames to be managed inside the Third-party Network A.

The inventory must have entries for the following hostnames:

 * *server-a1.net-a*
 * *server-a2.net-a*
 * *server-a3.net-a*

[IMPORTANT]
====
AAP should be able to resolve these hostnames, as they have been made available as services on the `*net-a*` namespace by {rhsi}, based on the `*Listeners*` created earlier.
====

==== Create the credential

The inventory has been created but you must also make sure you have defined the appropriate credential on the {aap} Console, so that it can connect with those hostnames through SSH.

==== Create the job template

We need to create a job template.

It will basically tie the Ansible project to be executed with the inventory that defines the target hosts and associate the credential to be used to access the Third-party Network A hosts.

Make sure your `*Job template*` is configured using the correct *Project*, *Inventory* and *Credential*.

At this point, all the AAP configuration needed is done.

=== Validating the scenario

Now that the whole scenario has been deployed, the *_Job template_* that was created on AAP should be able to run and reach all servers.

Run your job template and you should be able to see a successful result, as in the illustrative example below.

image::walkthrough_aap_workflow_success.png[Workflow job template successful]

== Conclusion

{rhsi} dramatically expands the reach of your centralized {aap} by securely managing previously inaccessible hosts.

Its user-friendly and declarative interface makes it easy to extend your automation to a wider range of devices, enabling you to manage hosts across hybrid and multi-cloud environments.
